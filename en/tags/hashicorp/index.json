[{"content":"I want to identify three methods in the industry under planning for capacity in the cloud.\nSRE principals include capacity planning in order to ensure production readiness for a release possibly using the current performance as a baseline. Many books go in depth about why you should deal with capacity, performance with tools and ways for your cloud infrastructure.\nThe first method is using monitoring tools to help guide your capacity plan when you understand the system’s needs. Here is a resource on the current landscape of observability. I’m not saying monitoring is observability, but here are some observability focused tools that help you achieve monitoring for everyone. We also get monitoring out of the box when we use cloud resources from a provider such as Amazon’s web services such as cloudwatch.\nThe second method typically known is benchmark testing with tools such as perfkitbenchmarker.\nBranden Greg’s system performance books discuss different methods for benchmarking in detail.\nHere are some free resources on these topics\nUSE Methodhttp://www.brendangregg.com/USEmethod/use-linux.html\nBenchmarking Methods https://link.springer.com/article/10.1186/2192-113X-2-6\nBenchmarking tools https://github.com/GoogleCloudPlatform/PerfKitBenchmarker\nOne reason not to use either of these methods which I believe method monitoring your application is the most important of the three and no excuse not to. The expenditures needed for the first two methods may be expensive. If your organization is new or is still in the process of maturing your delivery cycle, this may feel overwhelming to commit to this work or even pay for increased features from a cloud offering.\nAnother reason is the vendor has explicit rules about benchmark testing on their platform.\nBenchmark testing may not be practical for distributed systems in the cloud when resources will vary for different instance sizes such as databases or a FIFO queuing service.\nAnother solution is to set quotas for your application. This method is established in Cloud providers. Google presents some well enough high level docs to reference. They identify two types of quota setting.\n  Rate quota such as API requests per day. This quota resets after a specified time, such as a minute or a day.   This may be possible for your inhouse built application if you design it well. If it is too complex to add it or the application you are servicing doesn\u0026rsquo;t support it, then consider an open source proxy such as envoy which has circuit breaking or rate limiting built in.\n Allocation quota such as the number of virtual machines or load balancers used by your project. This quota does not reset over time but must be explicitly released when you no longer want to use the resource, for example by deleting a GKE cluster.   The ease and affordability of the allocation solution is attractive. Let\u0026rsquo;s say we have an application that stores data in memory, and periodically flushes memory to disk for recovery, and provides users to perform reads on the data in memory.\nA thought for using the monitoring method to monitor the size in memory of the machine. Allocate enough for realtime, the snapshot process and room for expensive queries. We want to ensure clear objectives if memory never goes over 70% during peak hours.\nWIth or without monitoring in place setting quotas will allow you to move from unplanned routine work of increasing storage for a database at the last possible minute to avoid downtime. Say a particular application is using 40% of your resources and they are instantaneously growing usage now you have an opportunity to speak with your teams about expansion plans and if there is an acceptable budget for both parties to work out of.\nThese all may be complex ways to design a system internally so you may think about controlling how many users to onboard for a service.\n","description":"Thoughts on capacity planning","id":2,"section":"posts","tags":["sre","cloud","delivery","capacity planning","monitoring","observability","benchmarking","open source"],"title":"Capacity Planning in the Cloud","uri":"https://alleeclark.github.io/en/posts/capacity-planning-vs-quotas/"},{"content":"Getting started with ttrpc\nI believe ttrpc stands for teeny tiny rpc. Don\u0026rsquo;t quote me on it. Created for low latency environments based off of the famous gRPC. I was introduced by ttrpc in a talk given by a containerD maintainer. Unexpected for you to know how containerD functions to get started with ttrpc. This guide is aimed for developers who know gRPC and would like to get started with ttrpc. Knowledge of gRPC isn\u0026rsquo;t required, however knowledge of golang and tcp is needed. I will felish out more of this tutorial for newbies to the rpc space soon. Important to note. ttrpc does sacrifice security features to reduce latency. I would not recommend opening up a ttrpc service to the world or external facing clients. I think it’s perfect for air gapped environments or already trusted inter service communication like a cache that runs in/with your service.\nRequirements  Golang gRPC requires 1.6 or higher Protoc Make sure you have gRPC installed  1  $ go get -u google.golang.org/grpc   Also the protoc plugin for Go\n1  $ go get -u github.com/golang/protobuf/protoc-gen-go   A thought out guide Next you will need to install ttrpc plugin for protoc\n1 2 3 4 5  $ go get github.com/containerd/ttrpc $ cd $GOPATH/src/github.com/containerd/ttrpc/cmd/protoc-gen-gogottrpc/ $ go install // verify the binary is $GOBIN $ ls $GOBIN // you should see \u0026#39;protoc-gen-gogottrpc\u0026#39;   Next create a package in your Go workspace\n1 2  $ mkdir $GOPATH/src/ttrpc-demo $ cd $GOPATH/src/ttrpc-demo   We\u0026rsquo;ll create a folder titled ‘pb’ to write our proto files in\n1 2  $ mkdir pb $ cd pb   Create a file titled hello.proto in the contents fo the file put\nsyntax = \u0026quot;proto3\u0026quot;; option go_package = \u0026quot;hello;hello\u0026quot;; service HelloService{ rpc HelloWorld(HelloRequest) returns (HelloResponse); } message HelloRequest{ string msg = 1; } message HelloResponse{ string response = 1; } Now we need to generate our code using the ttrpc plugin we installed earlier. This will create a new directory titled hello with generated code for your service.\n1  $ protoc -I ../pb/ --gogottrpc_out=plugins=ttrpc:../pb ../pb/*.proto\u0026lt;/div\u0026gt;   Lets write some server side code. Create a new directory titled server/ttrpc/ and a file titled server.go\n1 2  $ mkdir -p $GOPATH/src/ttrpc-demo/server/ttrpc $ touch $GOPATH/src/ttrpc-demo/server/ttrpc/server.go   In the server.go file, lets write our service calls\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  package main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;ttrpc-demo/pb/hello\u0026#34; \u0026#34;github.com/containerd/ttrpc\u0026#34; ) const port = \u0026#34;:9000\u0026#34; func main() { s, err := ttrpc.NewServer() defer s.Close() if err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } lis, err := net.Listen(\u0026#34;tcp\u0026#34;, port) if err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } hello.RegisterHelloServiceService(s, \u0026amp;helloService{}) if err := s.Serve(context.Background(), lis); err != nil { fmt.Fprintln(os.Stderr, err) } } type helloService struct{} func (s helloService) HelloWorld(ctx context.Context, r *hello.HelloRequest) (*hello.HelloResponse, error) { if r.Msg == \u0026#34;\u0026#34; { return nil, errors.New(\u0026#34;ErrNoInputMsgGiven\u0026#34;) } return \u0026amp;hello.HelloResponse{Response: \u0026#34;Hi How are you\u0026#34;}, nil }   Now lets generate our client side code. Let\u0026rsquo;s create a new directory and a client.go file\n1 2  $ mkdir -p $GOPATH/src/ttrpc-demo/client/ttrpc/ $ touch $GOPATH/src/ttrpc-demo/client/ttrpc/client.go   Inside client.go – place contents below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;ttrpc-demo/pb/hello\u0026#34; \u0026#34;github.com/containerd/ttrpc\u0026#34; ) const port = \u0026#34;:9001\u0026#34; func main() { conn, err := net.Dial(\u0026#34;tcp\u0026#34;, port) if err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Failed to dial: %v \\n\u0026#34;, err) os.Exit(1) } client := hello.NewHelloServiceClient(ttrpc.NewClient(conn)) serverResponse, err := client.HelloWorld(context.Background(), \u0026amp;hello.HelloRequest{ Msg: \u0026#34;Hello Server\u0026#34;, }) if err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } fmt.Fprintln(os.Stdout, serverResponse.Response) }   ","description":"Use a low latency rpc framework","id":5,"section":"posts","tags":["go","rpc"],"title":"Getting Started With ttrpc","uri":"https://alleeclark.github.io/en/posts/getting-started-with-ttrpc/"},{"content":"Getting started with go getter\nThis post is a small example of how to use Hashicorp\u0026rsquo;s go-getter. This package allows you to pull files easily from GCS, GIT or S3. I\u0026rsquo;ll explain a bit about my use case below. I assume you know about Hashicorp and golang and looking for a clean example to get started. For the past few weeks, I\u0026rsquo;ve written designs for a delivery system using Hashi(Hashicorp) tools such as Consul for keys and service discovery, Vault for dynamic secrets, and Terraform to create and modify infrastructure resources. I\u0026rsquo;m determining where I should store Terraform files and directories. I\u0026rsquo;m experimenting allowing clients to commit Terraform folders to a repository such as git or s3. Committing a Terraform directory creates a new infrastructure request to eventually get to the terraform apply phase. This may also provide a testing path to test out new terraform modules, upgrades, etc. The create request will clone the folder to a remote server and run terraform init, terraform plan, terraform apply. Below is an example of using the “GitGetter” for retrieving a subdirectory of a git repository using go-getter. This package is also an easier interface to git commands instead of dealing with the c binding for the git2go package. I haven\u0026rsquo;t read through all the code, however I believe go-getter clones the entire respoitory in memory and then returns you the directories you specify. In Part two I will add an example of invoking Terraform directly using it\u0026rsquo;s packages.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; getter \u0026#34;github.com/hashicorp/go-getter\u0026#34; ) func main() { client := \u0026amp;getter.Client{ Ctx: context.Background(), //define the destination to where the directory will be stored. This will create the directory if it doesnt exist \tDst: \u0026#34;/tmp/gogetter\u0026#34;, Dir: true, //the repository with a subdirectory I would like to clone only \tSrc: \u0026#34;github.com/hashicorp/terraform/examples/cross-provider\u0026#34;, Mode: getter.ClientModeDir, //define the type of detectors go getter should use, in this case only github is needed \tDetectors: []getter.Detector{ \u0026amp;getter.GitHubDetector{}, }, //provide the getter needed to download the files \tGetters: map[string]getter.Getter{ \u0026#34;git\u0026#34;: \u0026amp;getter.GitGetter{}, }, } //download the files \tif err := client.Get(); err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Error getting path %s: %v\u0026#34;, client.Src, err) os.Exit(1) } //now you should check your temp directory for the files to see if they exist }   ","description":"A small example of HashiCorp's GoGetter","id":6,"section":"posts","tags":["go","hashicorp","open source","cloud"],"title":"Getting Started With Go Getter","uri":"https://alleeclark.github.io/en/posts/getting-started-with-go-getter/"},{"content":"More than likely I am a Microsoft developer tool advocate. My first language was c++ using Visual Studio. Haven’t stopped sipping the kool aid since. Even though my primary language is golang today.\nBookmarks Same principle as your web browser. Keep track of places in the code that you know you will need to come back to.\nPros:\n Label bookmarks Side view for all bookmarks per file Search bookmarks Cons: Unable to add multiple lines per a bookmark.  Zen Mode Full screen for your editor. VSCode references zen mode as distraction free mode. No link available, however here is how to enable. Zen Mode: [View \u0026gt; Appearance \u0026gt; Toggle Zen Mode] Center Layout: [View \u0026gt; Appearance \u0026gt; Toggle Centered Layout]\nPros:\n Minimize distractions of other windows Cons: * Split screen unapplicable. Difficult to view other windows if you don not have multiple monitors. Remember good developers copy. Great developers paste. – Kelsey Hightower You must configure or know standard key bindings to search for files or toggle sidebar panels. * Integrated terminal doesn’t center  Workspaces Workspaces provides a way to work on and view multiple projects at one time. Even if your company uses a monorepo, that is a great way to minimize how many directories you have to click up or down in the side view.\nMemos My most used extension. For example use of tracking progress of a current coding task. At times I get walk ups from coworkers or have to attend a meeting. Sadly, I\u0026rsquo;m slightly extroverted than the average introvert, so I typically write down my last train of thought. When I get back to working I have a memo where I left off. Memos also help with my tab problem. If I find an interesting blog post, documentation or tracking an open source bug issue i\u0026rsquo;ll leave it in a memo.\nPros:\n Good for meeting not tracking. Don’t have to use another application Supports markdown Searchable  Gitlens The ultimate git cheat mode in the UI.\nPros:\n Visualize track your previous changes Visual other revisions * View commit history Search commit history Visualize who to blame  Code CLI Few maybe unfamiliar, but you may use $ code In the command line to perform actions such as launching a new window for a directory or file. Or just launching the editor front he terminal in general.\nPeacock Allows you to set the color of your Vscode workspace. If you develop in multiple repositories or normally have multiple instances of Vscode open, then this allows you to quickly distinguish between them.\n","description":"Extensions and features of vscode I enjoy","id":7,"section":"posts","tags":["time management","coding","vscode","editor"],"title":"VSCode Productivity Tips \u0026 Tricks","uri":"https://alleeclark.github.io/en/posts/vscode-tips-and-tricks/"},{"content":"The schedule below isn’t the full picture because I normally go on call every few weeks. I spend on call days communicating with other teams and resolving incidents. Bi-weekly attending meetings related to on-boarding, recruiting tech talks, and all hands. I only read work email in the morning unless someone PMs me to review. My company also uses Slack for communication and I limit my time on there as well. This facilitates minimizing distractions and intaking too much information. Days when I’m physically tired stem from being mentally tire from context switching. It is difficult to reach me throughout the day, if it wasn’t scheduled in advance.\n   Timestamp Activity     08:00-08:15 AM Get breakfast   8:15-9:00 AM Eat breakfast and bulk read and draft response emails.   9:00-10:00 AM Attempt to fix any blockers from yesterday and review code of coworkers.   10:00-10:15 AM Standup with team or communicate what I'm working on or blocked by   10:15-10:25 AM Quick walk – possibly get a snack   10:25-11:50 AM Write code, research, document   11:50- 12:45 PM Some days lunch is provided and impacts how much time in spend at lunch. I'll give it 55 minutes   12:45-1:30 PM Write code, research, document   1:30-2:00 PM 1:1 with someone   2:00-3:20 PM Write code, research, document   3:20-3:35 PM A little break might go outside   3:35-5:00 PM or EOD Write code, research, document. Depends on how motivated I feel to finish task   Last 10 minutes Retrospective and thoughts for tomorrow    Writing code, researching and documenting consist of anything between code reviews, writing configuration, rpc services, dashboarding, design docs, etc. If I arrive late or early, the schedule adjust. Most teams perform retros at the end of a sprint. I peform self retros daily since I look to complete a task per a day. I am able to easily remember what I did today and how I felt about it. It sucks being in a retro two weeks later, and having to dig through my thoughts to remember what I worked on and what went wrong. My downfall at times is not ending the day when I know I should. I\u0026rsquo;ve started setting goals outside of work that begins in the evenings. When I\u0026rsquo;m not on call I even leave my laptop at works.\n","description":"How I schedule my day","id":8,"section":"posts","tags":["time management"],"title":"My Daily Work Routine","uri":"https://alleeclark.github.io/en/posts/daily-sre-routine/"},{"content":"This is mainly written from a production engineer’s view. This might be helpful for an engineer who is response for some production service, and just wants a sanity check… On with the question. Sometimes you have legacy software that most of your company’s critical path relies on. That software hasn\u0026rsquo;t powered off in X amount of scary years. Some combination of documentation, automation of task and source control code is nonexistent. One question to ask is — when was the last time this box (host machine) powered off? Do you know what it takes to restart process(es). Are you positive you can restart all the processes on that host if you powered it off and used that restart it trick? If the answer is no, you my friend have some (pet instances). Is the recovery plan well documented? Actionable documentation is key. The documentation should contain what to do. The dependent services or jobs that might need a restart, kill, relaunch, rescheduled. The proper sequence of task execution. Consumers and the stakeholders of the system? I\u0026rsquo;m going overkill but you get the point. What does data integrity and retention policies exist for this system? Sometimes there will be DB locks that are handling payment operations at the application layer, inoperable code that will cause memleaks or hardware failures. Is it okay to lose any of that data? I say yes it is okay to lose data. If not of course there are more expensive solutions to deploy. But you can’t think about that now. If the incident is in a domain that is out of your comfort zone with solving, than use the appropriate escalation path. Don’t just say is anyone getting a 400 error? Describe the service, where it is failing and any other key details.\nHow important are user experiences? How are they experiencing the current incident? Not thinking how but actually engaging and getting input from your users if possible? If these are services that your users interact with, or supports internal employees to help customers have those same experiences when they call in are vital. Execute this well. Then everyone will feel great about the decisions made at the resolution. I suggest codifying as much as you can during the incident regardless if there wasn\u0026rsquo;t anything before. I can see checking logs and restarting processes. If you are starting process(es) with different flag options, or installing dependencies; than you should code those steps and document them. That way you can have a changelog for the postmortem. The best bet is to have the code reviewed during the incident by a team member. Reviewed code helps show consensus on the path to resolution. Also if you have to step away from the incident or the incident occurs again, someone else may pick it up from your train of thought. Through this process you are improving your code base, and removing technical debt during an incident. Don’t forget to check in on the users affected . Inform them of any updates to the system and if there is any loss of data. Communication is key and make sure you consistently provide updates. Users will always inquire for an ETA. Sometimes you will be unsure. So give a time period of when you will provide the next update. 30 minutes is usually always fair unless you have strict SLAs. After this incident is over write a postmortem (Michael’s is the best IMO, and he’s a cool SRE) and publish it for the company. Don\u0026rsquo;t have a Post morteum Culture? bring It. Send it out as an email to your organization as a technical thought. Set up a meeting with an optional invite that people can join, and learn more about your experiences and the systems that exist.\nBut guess what. Now you know more about how that shit works. ","description":"","id":9,"section":"posts","tags":["sre"],"title":"What decision should I make about an incident?","uri":"https://alleeclark.github.io/en/posts/incident-decisions/"},{"content":"I am Allee. Computer scientist, SRE, writer and speaker. Dynamic credentials, service discovery and containers are a few of my favorite things. I love cars and food.\nI\u0026rsquo;ll Share technical thoughts and post reviews of books and conferences.\nA good way to set up a 1x1 is here.\n//allee\n","description":"","id":10,"section":"","tags":null,"title":"About","uri":"https://alleeclark.github.io/en/about/"}]